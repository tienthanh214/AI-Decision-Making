# -------------- IMPORT PACKAGE ------------------
# import Pkg

# import_packages = ["Plots", "Ipopt", "JuMP", "Distributions", "LinearAlgebra", "JLD", "Plots"]
# for pkg in import_packages
#     if !haskey(Pkg.installed(), pkg)
#         Pkg.add(pkg)
#     end
# end

# if !haskey(Pkg.installed(), "DecisionMakingProblems")
#     Pkg.add(url = "https://github.com/algorithmsbooks/DecisionMakingProblems.jl")
# end

import Base.Iterators: product
using JuMP, Ipopt, Distributions, LinearAlgebra, JLD, Plots
using DecisionMakingProblems

struct SetCategorical{S}
    elements::Vector{S} # Set elements (could be repeated)
    distr::Categorical # Categorical distribution over set elements
    
    function SetCategorical(elements::AbstractVector{S}) where S
        weights = ones(length(elements))
        return new{S}(elements, Categorical(normalize(weights, 1)))
    end

    function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where S
        ‚Ñì‚ÇÅ = norm(weights,1)
        if ‚Ñì‚ÇÅ < 1e-6 || isinf(‚Ñì‚ÇÅ)
            return SetCategorical(elements)
        end
        distr = Categorical(normalize(weights, 1))
        return new{S}(elements, distr)
    end
end

Distributions.rand(D::SetCategorical) = D.elements[rand(D.distr)]
Distributions.rand(D::SetCategorical, n::Int) = D.elements[rand(D.distr, n)]

    function Distributions.pdf(D::SetCategorical, x)
    sum(e == x ? w : 0.0 for (e,w) in zip(D.elements, D.distr.p))
end

# -------------- POLICY ------------------
struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))
    end
    
    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end

(œÄi::SimpleGamePolicy)(ai) = get(œÄi.p, ai, 0.0)

function (œÄi::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(œÄi.p)), collect(values(œÄi.p)))
    return rand(D)
end

joint(X) = vec(collect(product(X...)))
joint(œÄ, œÄi, i) = [i == j ? œÄi : œÄj for (j, œÄj) in enumerate(œÄ)]

function utility(ùí´::SimpleGame, œÄ, i)
    ùíú, R = ùí´.ùíú, ùí´.R
    p(a) = prod(œÄj(aj) for (œÄj, aj) in zip(œÄ, a))
    return sum(R(a)[i]*p(a) for a in joint(ùíú))
end

struct MGPolicy
    p # dictionary mapping states to simple game policies
    MGPolicy(p::Base.Generator) = new(Dict(p))
end

(œÄi::MGPolicy)(s, ai) = œÄi.p[s](ai)
(œÄi::SimpleGamePolicy)(s, ai) = œÄi(ai)

probability(ùí´::MG, s, œÄ, a) = prod(œÄj(s, aj) for (œÄj, aj) in zip(œÄ, a))
reward(ùí´::MG, s, œÄ, i) =
    sum(ùí´.R(s,a)[i]*probability(ùí´,s,œÄ,a) for a in joint(ùí´.ùíú))
transition(ùí´::MG, s, œÄ, s‚Ä≤) =
    sum(ùí´.T(s,a,s‚Ä≤)*probability(ùí´,s,œÄ,a) for a in joint(ùí´.ùíú))

function policy_evaluation(ùí´::MG, œÄ, i)
    ùíÆ, ùíú, R, T, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T, ùí´.Œ≥
    p(s,a) = prod(œÄj(s, aj) for (œÄj, aj) in zip(œÄ, a))
    R‚Ä≤ = [sum(R(s,a)[i]*p(s,a) for a in joint(ùíú)) for s in ùíÆ]
    T‚Ä≤ = [sum(T(s,a,s‚Ä≤)*p(s,a) for a in joint(ùíú)) for s in ùíÆ, s‚Ä≤ in ùíÆ]
    return (I - Œ≥*T‚Ä≤)\R‚Ä≤
end

# -------------- NASH EQUILIBRIUM ------------------
struct NashEquilibrium end

function tensorform(ùí´::MG)
    ‚Ñê, ùíÆ, ùíú, R, T = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.T
    ‚Ñê‚Ä≤ = eachindex(‚Ñê)
    ùíÆ‚Ä≤ = eachindex(ùíÆ)
    ùíú‚Ä≤ = [eachindex(ùíú[i]) for i in ‚Ñê]
    R‚Ä≤ = [R(s,a) for s in ùíÆ, a in joint(ùíú)]
    T‚Ä≤ = [T(s,a,s‚Ä≤) for s in ùíÆ, a in joint(ùíú), s‚Ä≤ in ùíÆ]
    return ‚Ñê‚Ä≤, ùíÆ‚Ä≤, ùíú‚Ä≤, R‚Ä≤, T‚Ä≤
end

function solve!(M::NashEquilibrium, ùí´::MG)
    ‚Ñê, ùíÆ, ùíú, R, T = tensorform(ùí´)
    ùíÆ‚Ä≤, ùíú‚Ä≤, Œ≥ = ùí´.ùíÆ, ùí´.ùíú, ùí´.Œ≥
    model = Model(Ipopt.Optimizer)
    @variable(model, U[‚Ñê, ùíÆ])
    print("c1")
    @variable(model, œÄ[i=‚Ñê, ùíÆ, ai=ùíú[i]] ‚â• 0)
    @NLobjective(model, Min,
        sum(U[i,s] - sum(prod(œÄ[j,s,a[j]] for j in ‚Ñê)
            * (R[s,y][i] + Œ≥*sum(T[s,y,s‚Ä≤]*U[i,s‚Ä≤] for s‚Ä≤ in ùíÆ))
            for (y,a) in enumerate(joint(ùíú))) for i in ‚Ñê, s in ùíÆ))
    @NLconstraint(model, [i=‚Ñê, s=ùíÆ, ai=ùíú[i]],
        U[i,s] ‚â• sum(
            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : œÄ[j,s,a[j]] for j in ‚Ñê)
            * (R[s,y][i] + Œ≥*sum(T[s,y,s‚Ä≤]*U[i,s‚Ä≤] for s‚Ä≤ in ùíÆ))
            for (y,a) in enumerate(joint(ùíú))))
    print("c2")
    @constraint(model, [i=‚Ñê, s=ùíÆ], sum(œÄ[i,s,ai] for ai in ùíú[i]) == 1)
    print("c3")
    optimize!(model)
    print("c4")
    œÄ‚Ä≤ = value.(œÄ)
    œÄi‚Ä≤(i,s) = SimpleGamePolicy(ùíú‚Ä≤[i][ai] => œÄ‚Ä≤[i,s,ai] for ai in ùíú[i])
    œÄi‚Ä≤(i) = MGPolicy(ùíÆ‚Ä≤[s] => œÄi‚Ä≤(i,s) for s in ùíÆ)
    return [œÄi‚Ä≤(i) for i in ‚Ñê]
end

# -------------- FIND NASH EQUILIBRIUM ------------------
pphw = PredatorPreyHexWorld()
mg = MG(pphw)
œÄ = solve!(NashEquilibrium(), mg)
# save("NE.jld", "NE.jld", œÄ)