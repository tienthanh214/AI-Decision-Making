# -------------- IMPORT PACKAGE ------------------
# import Pkg

# import_packages = ["Plots", "Ipopt", "JuMP", "Distributions", "LinearAlgebra", "JLD", "Plots"]
# for pkg in import_packages
#     if !haskey(Pkg.installed(), pkg)
#         Pkg.add(pkg)
#     end
# end

# if !haskey(Pkg.installed(), "DecisionMakingProblems")
#     Pkg.add(url = "https://github.com/algorithmsbooks/DecisionMakingProblems.jl")
# end

using Suppressor
import Base.Iterators: product
using JuMP, Ipopt, Distributions, LinearAlgebra, JLD, Plots
using DecisionMakingProblems

struct SetCategorical{S}
    elements::Vector{S} # Set elements (could be repeated)
    distr::Categorical # Categorical distribution over set elements
    
    function SetCategorical(elements::AbstractVector{S}) where S
        weights = ones(length(elements))
        return new{S}(elements, Categorical(normalize(weights, 1)))
    end

    function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where S
        â„“â‚ = norm(weights,1)
        if â„“â‚ < 1e-6 || isinf(â„“â‚)
            return SetCategorical(elements)
        end
        distr = Categorical(normalize(weights, 1))
        return new{S}(elements, distr)
    end
end

Distributions.rand(D::SetCategorical) = D.elements[rand(D.distr)]
Distributions.rand(D::SetCategorical, n::Int) = D.elements[rand(D.distr, n)]

    function Distributions.pdf(D::SetCategorical, x)
    sum(e == x ? w : 0.0 for (e,w) in zip(D.elements, D.distr.p))
end

# -------------- POLICY ------------------
struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))
    end
    
    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end

(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)

function (Ï€i::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
    return rand(D)
end

joint(X) = vec(collect(product(X...)))
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]

function utility(ğ’«::SimpleGame, Ï€, i)
    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
    return sum(R(a)[i]*p(a) for a in joint(ğ’œ))
end

struct MGPolicy
    p # dictionary mapping states to simple game policies
    MGPolicy(p::Base.Generator) = new(Dict(p))
end

(Ï€i::MGPolicy)(s, ai) = Ï€i.p[s](ai)
(Ï€i::SimpleGamePolicy)(s, ai) = Ï€i(ai)

probability(ğ’«::MG, s, Ï€, a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))
reward(ğ’«::MG, s, Ï€, i) =
    sum(ğ’«.R(s,a)[i]*probability(ğ’«,s,Ï€,a) for a in joint(ğ’«.ğ’œ))
transition(ğ’«::MG, s, Ï€, sâ€²) =
    sum(ğ’«.T(s,a,sâ€²)*probability(ğ’«,s,Ï€,a) for a in joint(ğ’«.ğ’œ))

function policy_evaluation(ğ’«::MG, Ï€, i)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    p(s,a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))
    Râ€² = [sum(R(s,a)[i]*p(s,a) for a in joint(ğ’œ)) for s in ğ’®]
    Tâ€² = [sum(T(s,a,sâ€²)*p(s,a) for a in joint(ğ’œ)) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³*Tâ€²)\Râ€²
end

# -------------- SIMULATE FOR LEARNING ------------------
function randstep(ğ’«::MG, s, a)
    sâ€² = rand(SetCategorical(ğ’«.ğ’®, [ğ’«.T(s, a, sâ€²) for sâ€² in ğ’«.ğ’®]))
    r = ğ’«.R(s,a)
    return sâ€², r
end

function simulate(ğ’«::MG, Ï€, start_state, k_max, k_reset)
    print("Start state: ", start_state, '\n')
    s = start_state
    for k = 1:k_max
        if k % 100 == 0
            print(k, '/', k_max, '\n')
        end
        a = Tuple(Ï€i(s)() for Ï€i in Ï€)
        sâ€², r = randstep(ğ’«, s, a)
        for i in collect(1:length(Ï€))
            update!(Ï€[i], s, a, sâ€²)
        end
        s = sâ€²
        if k % k_reset == 0
            s = start_state
        end
    end
    return Ï€
end

# -------------- NASH Q-LEARNING ------------------
struct NashEquilibrium end

function tensorform(ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    â„â€² = eachindex(â„)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(a) for a in joint(ğ’œ)]
    return â„â€², ğ’œâ€², Râ€²
end

function solveSG(M::NashEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„])
    @variable(model, Ï€[i=â„, ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
        sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]
            for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„, ai=ğ’œ[i]],
        U[i] â‰¥ sum(
            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)
            * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai] => value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end

mutable struct NashQLearning
    ğ’« # Markov game
    i # agent index
    Q # state-action value estimates
    N # history of actions performed
end

function NashQLearning(ğ’«::MG, i)
    â„, ğ’®, ğ’œ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ
    Q = Dict((j, s, a) => 0.0 for j in â„, s in ğ’®, a in joint(ğ’œ))
    N = Dict((s, a) => 1.0 for s in ğ’®, a in joint(ğ’œ))
    return NashQLearning(ğ’«, i, Q, N)
end

function (Ï€i::NashQLearning)(s)
    ğ’«, i, Q, N = Ï€i.ğ’«, Ï€i.i, Ï€i.Q, Ï€i.N
    â„, ğ’®, ğ’œ, ğ’œi, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’œ[Ï€i.i], ğ’«.Î³
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, a -> [Q[j, s, a] for j in â„])
    Ï€ = solveSG(M, ğ’¢)
    Ïµ = 1 / sum(N[s, a] for a in joint(ğ’œ))
    Ï€iâ€²(ai) = Ïµ/length(ğ’œi) + (1-Ïµ)*Ï€[i](ai)
    return SimpleGamePolicy(ai => Ï€iâ€²(ai) for ai in ğ’œi)
end

function update!(Ï€i::NashQLearning, s, a, sâ€²)
    ğ’«, â„, ğ’®, ğ’œ, R, Î³ = Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.ğ’«.ğ’®, Ï€i.ğ’«.ğ’œ, Ï€i.ğ’«.R, Ï€i.ğ’«.Î³
    i, Q, N = Ï€i.i, Ï€i.Q, Ï€i.N
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, aâ€² -> [Q[j, sâ€², aâ€²] for j in â„])
    Ï€ = solveSG(M, ğ’¢)
    Ï€i.N[s, a] += 1
    Î± = 1 / sqrt(N[s, a])
    for j in â„
        Ï€i.Q[j,s,a] += Î±*(R(s,a)[j] + Î³*utility(ğ’¢,Ï€,j) - Q[j,s,a])
    end
end

# -------------- NASH Q-LEARNING SIMULATING ------------------

function NQLtoMGPolicy(ğ’«::MG, Ï€i::NashQLearning)
    return MGPolicy(s => Ï€i(s) for s in ğ’«.ğ’®)
end

function nash_Qlearning(PPHW::DecisionMakingProblems.PredatorPreyHexWorldMG, k_max)
    ğ’« = MG(PPHW)
    Ï€ = [NashQLearning(ğ’«, i) for i in ğ’«.â„]
    cnt = 0
    for s in ğ’«.ğ’®
        cnt += 1
        if s[1] == s[2]
            continue
        end
        if s != (3, 10)
            continue
        end
        print(cnt, '/', length(ğ’«.ğ’®), '\n')
        Ï€ = simulate(ğ’«, Ï€, s, k_max, 10)
    end
    Ï€ = [NQLtoMGPolicy(ğ’«, Ï€i) for Ï€i in Ï€]
    return Ï€
end

pphw = PredatorPreyHexWorld()
ğ’« = MG(pphw)
Ï€ = nash_Qlearning(pphw, 300)
save("trained_NQL.jld", "trained_pi", Ï€)