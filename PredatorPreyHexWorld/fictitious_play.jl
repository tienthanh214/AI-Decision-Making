# main algorithm for Predator-Prey Hex World problem

# -------------- IMPORT PACKAGE AND INCLUDE ------------------
import Pkg

import_packages = ["Ipopt", "JuMP", "Distributions", "LinearAlgebra", "JLD", "Plots"]
for pkg in import_packages
    if !haskey(Pkg.installed(), pkg)
        Pkg.add(pkg)
    end
end

if !haskey(Pkg.installed(), "DecisionMakingProblems")
    Pkg.add(url = "https://github.com/algorithmsbooks/DecisionMakingProblems.jl")
end

using JLD
using DecisionMakingProblems
include("simple_game.jl")
include("markov_game.jl")

# -------------- SIMULATE FOR LEARNING ------------------
function randstep(ğ’«::MG, s, a)
    # Create a randomized step base on action 
    sâ€² = rand(SetCategorical(ğ’«.ğ’®, [ğ’«.T(s, a, sâ€²) for sâ€² in ğ’«.ğ’®]))
    r = ğ’«.R(s,a)
    return sâ€², r
end

function simulate!(ğ’«::MG, Ï€, start_state, k_max; k_reset = 0)
    # Simulate for learning-based algorithms
    s = start_state
    for k = 1:k_max
        a = Tuple(Ï€i(s)() for Ï€i in Ï€)
        sâ€², r = randstep(ğ’«, s, a)
        for i in collect(1:length(Ï€))
            update!(Ï€[i], s, a, sâ€²)
        end
        s = sâ€²
        if (k_reset != 0 && k % k_reset == 0)
            s = start_state
        end
    end
    return Ï€
end

# -------------- FICTITIOUS PLAY ------------------
mutable struct MGFictitiousPlay
    # Definition for Fictitious Play
    ğ’« # Markov game
    i # agent index
    Qi # state-action value estimates
    Ni # state-action counts
end

function MGFictitiousPlay(ğ’«::MG, i)
    # Construct FP for agent i
    â„, ğ’®, ğ’œ, R = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    Qi = Dict((s, a) => R(s, a)[i] for s in ğ’® for a in joint(ğ’œ))
    Ni = Dict((j, s, aj) => 1.0 for j in â„ for s in ğ’® for aj in ğ’œ[j])
    return MGFictitiousPlay(ğ’«, i, Qi, Ni)
end

function (Ï€i::MGFictitiousPlay)(s)
    # Return a SimpleGamePolicy for state s, based on uitility
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    Ï€iâ€²(i,s) = SimpleGamePolicy(ai => Ï€i.Ni[i,s,ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i,s) for s in ğ’®)
    Ï€ = [Ï€iâ€²(i) for i in â„]
    U(s,Ï€) = sum(Ï€i.Qi[s,a]*probability(ğ’«,s,Ï€,a) for a in joint(ğ’œ))
    Q(s,Ï€) = reward(ğ’«,s,Ï€,i) + Î³*sum(transition(ğ’«,s,Ï€,sâ€²)*U(sâ€²,Ï€) for sâ€² in ğ’®)
    Q(ai) = Q(s, joint(Ï€, SimpleGamePolicy(ai), i))
    ai = argmax(Q, ğ’«.ğ’œ[Ï€i.i])
    return SimpleGamePolicy(ai)
end

function update!(Ï€i::MGFictitiousPlay, s, a, sâ€²)
    # Update Fictitious Play based on simulated actions
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    for (j,aj) in enumerate(a)
        Ï€i.Ni[j,s,aj] += 1
    end
    Ï€iâ€²(i,s) = SimpleGamePolicy(ai => Ï€i.Ni[i,s,ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i,s) for s in ğ’®)
    Ï€ = [Ï€iâ€²(i) for i in â„]
    U(Ï€,s) = sum(Ï€i.Qi[s,a]*probability(ğ’«,s,Ï€,a) for a in joint(ğ’œ))
    Q(s,a) = R(s,a)[i] + Î³*sum(T(s,a,sâ€²)*U(Ï€,sâ€²) for sâ€² in ğ’®)
    for a in joint(ğ’œ)
        Ï€i.Qi[s,a] = Q(s,a)
    end
end

# -------------- FICTITIOUS PLAY SIMULATING ------------------
function MGFPtoMGPolicy(ğ’«::MG, Ï€i::MGFictitiousPlay)
    # Translate from MGFictitiousPlay to MGPolicy
    return MGPolicy(s => Ï€i(s) for s in ğ’«.ğ’®)
end

function fictitious_play(pphw::DecisionMakingProblems.PredatorPreyHexWorldMG, k_max)
    # Concurrent simluating for Fititiious Play algorithm
    ğ’« = MG(pphw)
    Ï€ = [MGFictitiousPlay(ğ’«, i) for i in ğ’«.â„]
    for i in collect(1:k_max)
        print("Iter: ", i, '/', k_max, '\n')
        for (i, s) in enumerate(ğ’«.ğ’®)
            if s[1] == s[2]
                continue
            end
            print(i, '/', length(ğ’«.ğ’®), '\n')
            Ï€ = simulate!(ğ’«, Ï€, s, 10)
        end
    end    
    Ï€ = [MGFPtoMGPolicy(ğ’«, Ï€i) for Ï€i in Ï€]
    return Ï€
end

# -------------- LEARNING ---------------------
pphw = PredatorPreyHexWorld()
ğ’« = MG(pphw)
Ï€ = fictitious_play(pphw, 30)
save("trained_policy/trained_FP.jld", "trained_pi", Ï€)