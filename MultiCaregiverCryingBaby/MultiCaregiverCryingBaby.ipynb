{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Caregiver Crying Baby\n",
    "Partially Observable Markov Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îå Warning: Pkg.installed() is deprecated\n",
      "‚îî @ Pkg C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\Pkg.jl:595\n",
      "‚îå Warning: Pkg.installed() is deprecated\n",
      "‚îî @ Pkg C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\Pkg.jl:595\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"JuMP\") \n",
    "    Pkg.add(\"JuMP\")\n",
    "end\n",
    "if !haskey(Pkg.installed(), \"Ipopt\")\n",
    "    Pkg.add(\"Ipopt\")\n",
    "end\n",
    "\n",
    "using JuMP, Ipopt, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SING = \"SING\"\n",
    "CRYING = \"CRYING\"\n",
    "QUIET = \"QUIET\"\n",
    "FEED = \"FEED\"\n",
    "SATED = \"SATED\"\n",
    "HUNGRY = \"HUNGRY\"\n",
    "\n",
    "p_cry_when_hungry_in_sing = 0.9\n",
    "p_cry_when_hungry = 0.9\n",
    "p_cry_when_not_hungry = 0.0\n",
    "p_become_hungry = 0.5\n",
    "\n",
    "r_hungry = 10.0\n",
    "r_sing = 0.5\n",
    "r_feed = 5.0\n",
    "\n",
    "struct POMG\n",
    "    Œ≥  # discount factor\n",
    "    ‚Ñê  # agents\n",
    "    ùíÆ  # state space\n",
    "    ùíú  # joint action space \n",
    "    ùí™  # joint observation space\n",
    "    T  # transition function\n",
    "    O  # joint observation function\n",
    "    R  # joint reward function\n",
    "\n",
    "    function POMG(discount, agents, states, jointAction, jointObservation, transitionFunc, jointObservationFunc, jointRewardFunc)\n",
    "        new(discount, agents, states, jointAction, jointObservation, transitionFunc, jointObservationFunc, jointRewardFunc)\n",
    "    end\n",
    "end\n",
    "\n",
    "struct ConditionalPlan\n",
    "    a   # action to take at root\n",
    "    subplans    # dictionary mapping observations to subplans \n",
    "end\n",
    "\n",
    "struct SimpleGame\n",
    "    Œ≥  # discount factor\n",
    "    ‚Ñê  # agents\n",
    "    ùíú  # joint action space\n",
    "    R  # joint reward function\n",
    "end\n",
    "\n",
    "struct NashEquilibrium end\n",
    "\n",
    "# The general structure of Simple game \n",
    "# source: Algorithms for Decision Making book\n",
    "struct SimpleGamePolicy\n",
    "    p # dictionary mapping actions to probabilities\n",
    "\n",
    "    # Returns a random policy\n",
    "    function SimpleGamePolicy(p::Base.Generator)\n",
    "        return SimpleGamePolicy(Dict(p))\n",
    "    end\n",
    "    # Return policy from dict\n",
    "    function SimpleGamePolicy(p::Dict)\n",
    "        vs = collect(values(p))\n",
    "        vs ./= sum(vs)\n",
    "        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))\n",
    "    end\n",
    "\n",
    "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
    "end\n",
    "\n",
    "ConditionalPlan(a) = ConditionalPlan(a, Dict())\n",
    "\n",
    "(œÄ::ConditionalPlan)() = œÄ.a\n",
    "(œÄ::ConditionalPlan)(o) = œÄ.subplans[o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transition (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transition(s, a, s‚Ä≤)\n",
    "    # Regardless, feeding makes the baby sated.\n",
    "    if a[1] == \"FEED\" || a[2] == \"FEED\" \n",
    "        if s‚Ä≤ == \"SATED\" \n",
    "            return 1.0\n",
    "        else \n",
    "            return 0.0\n",
    "        end\n",
    "    else\n",
    "        # If neither caretaker feed, then one of two things happens.\n",
    "        # First, a baby that is hungry remains hungry \n",
    "        if s == \"HUNGRY\"\n",
    "            if s‚Ä≤ == \"HUNGRY\"\n",
    "                return 1.0\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        # Otherwise, it becomes hungry with a fixed probability.\n",
    "        else\n",
    "            if s‚Ä≤ == \"SATED\"\n",
    "                return 1.0 - p_become_hungry\n",
    "            else\n",
    "                return p_become_hungry\n",
    "            end \n",
    "        end \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joint_observation (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function joint_observation(a, s‚Ä≤, o)\n",
    "    # If at least one caregiver sings, then both observe the result.\n",
    "    if a[1] == \"SING\" || a[2] == \"SING\"\n",
    "        # If the baby is hungry, then the caregivers both observe crying/silent together.\n",
    "        if s‚Ä≤ == \"HUNGRY\"\n",
    "            if o[1] == \"CRYING\" && o[2] == \"CRYING\"\n",
    "                return p_cry_when_hungry_in_sing\n",
    "            elseif o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0 - p_cry_when_hungry_in_sing\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        # Otherwise the baby is sated\n",
    "        else\n",
    "            if o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        end\n",
    "    # Otherwise the caregivers fed and/or ignored the baby\n",
    "    else \n",
    "        # If the baby is hungry, then there‚Ä≤s a probability it cries\n",
    "        if s‚Ä≤ == \"HUNGRY\"\n",
    "            if o[1] == \"CRYING\" && o[2] == \"CRYING\"\n",
    "                return p_cry_when_hungry \n",
    "            elseif o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0 - p_cry_when_hungry\n",
    "            else \n",
    "                return 0.0\n",
    "            end \n",
    "        # If the baby is sated, then there‚Ä≤s no probability it cries\n",
    "        else\n",
    "            if o[1] == \"CRYING\" && o[2] == \"CRYING\" \n",
    "                return p_cry_when_not_hungry\n",
    "            elseif o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0 - p_cry_when_not_hungry\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        end \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joint_reward (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function joint_reward(s, a) \n",
    "    r = [0.0, 0.0]\n",
    "    \n",
    "    # Both caregivers do not want the child to be hungry\n",
    "    if s == \"HUNGRY\"\n",
    "        r -= [r_hungry, r_hungry]\n",
    "    end\n",
    "\n",
    "    # the first caregiver favors feeding \n",
    "    if a[1] == \"FEED\" \n",
    "        r[1] -= r_feed / 2.0 \n",
    "    elseif a[1] == \"SING\"\n",
    "        r[1] -= r_sing\n",
    "    end\n",
    "\n",
    "    # the second caregiver favors singing\n",
    "    if a[2] == \"SING\"\n",
    "        r[2] -= r_sing / 2\n",
    "    elseif a[2] == \"FEED\"\n",
    "        r[2] -= r_feed\n",
    "    end\n",
    "    \n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Conditional Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utility (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The lookahead function below is used to calculate the evaluate plan\n",
    "function lookahead(ùí´::POMG, U, s, a) \n",
    "    ùíÆ, ùí™, T, O, R, Œ≥ = ùí´.ùíÆ, joint(ùí´.ùí™), ùí´.T, ùí´.O, ùí´.R, ùí´.Œ≥\n",
    "    u‚Ä≤ = sum(T(s,a,s‚Ä≤)*sum(O(a,s‚Ä≤,o)*U(o,s‚Ä≤) for o in ùí™) for s‚Ä≤ in ùíÆ)\n",
    "    return R(s,a) + Œ≥*u‚Ä≤\n",
    "end\n",
    "\n",
    "#  The lookahead function below is used to calculate the utility\n",
    "function evaluate_plan(ùí´::POMG, œÄ, s)\n",
    "    a = Tuple(œÄi() for œÄi in œÄ)\n",
    "    U(o,s‚Ä≤) = evaluate_plan(ùí´, [œÄi(oi) for (œÄi, oi) in zip(œÄ,o)], s‚Ä≤)\n",
    "    return isempty(first(œÄ).subplans) ? ùí´.R(s,a) : lookahead(ùí´, U, s, a)\n",
    "end\n",
    "\n",
    "# used to calculate utility with initial belief b when executing joint policy in POMG ùí´\n",
    "function utility(ùí´::POMG, b, œÄ)\n",
    "    u = [evaluate_plan(ùí´, œÄ, s) for s in ùí´.ùíÆ]\n",
    "    return sum(bs * us for (bs, us) in zip(b, u))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash Equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function expand_conditional_plans(ùí´, Œ†)\n",
    "    ‚Ñê, ùíú, ùí™ = ùí´.‚Ñê, ùí´.ùíú, ùí´.ùí™\n",
    "    return [[ConditionalPlan(ai, Dict(oi => œÄi for oi in ùí™[i]))\n",
    "        for œÄi in Œ†[i] for ai in ùíú[i]] for i in ‚Ñê]\n",
    "end\n",
    "\n",
    "joint(X) = vec(collect(Iterators.product(X...)))\n",
    "joint(œÄ, œÄi, i) = [i == j ? œÄi : œÄj for (j, œÄj) in enumerate(œÄ)]\n",
    "\n",
    "# Returns the format tensor of ùí´\n",
    "function tensorform(ùí´::SimpleGame)\n",
    "    ‚Ñê, ùíú, R = ùí´.‚Ñê, ùí´.ùíú, ùí´.R\n",
    "    ‚Ñê‚Ä≤ = eachindex(‚Ñê)\n",
    "    ùíú‚Ä≤ = [eachindex(ùíú[i]) for i in ‚Ñê]\n",
    "    R‚Ä≤ = [R(a) for a in joint(ùíú)]\n",
    "    return ‚Ñê‚Ä≤, ùíú‚Ä≤, R‚Ä≤\n",
    "end\n",
    "\n",
    "# Find the Nash Equilibrium\n",
    "function solve(M::NashEquilibrium, ùí´::SimpleGame)\n",
    "    ‚Ñê, ùíú, R = tensorform(ùí´)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    #  declaration\n",
    "    @variable(model, U[‚Ñê])\n",
    "    # constraint 3\n",
    "    @variable(model, œÄ[i=‚Ñê, ùíú[i]] ‚â• 0)\n",
    "    # objective function\n",
    "    @NLobjective(model, Min,\n",
    "        sum(U[i] - sum(prod(œÄ[j,a[j]] for j in ‚Ñê) * R[y][i]\n",
    "            for (y,a) in enumerate(joint(ùíú))) for i in ‚Ñê))\n",
    "    # constraint 1\n",
    "    @NLconstraint(model, [i=‚Ñê, ai=ùíú[i]],\n",
    "        U[i] ‚â• sum(\n",
    "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : œÄ[j,a[j]] for j in ‚Ñê)\n",
    "            * R[y][i] for (y,a) in enumerate(joint(ùíú))))\n",
    "    # constrain 2\n",
    "    @constraint(model, [i=‚Ñê], sum(œÄ[i,ai] for ai in ùíú[i]) == 1)\n",
    "    # Model optimization\n",
    "    optimize!(model)\n",
    "    œÄi‚Ä≤(i) = SimpleGamePolicy(ùí´.ùíú[i][ai] => value(œÄ[i,ai]) for ai in ùíú[i])\n",
    "    return [œÄi‚Ä≤(i) for i in ‚Ñê]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct POMGDynamicProgramming\n",
    "    b # initial belief\n",
    "    d # depth of conditional plans\n",
    "end\n",
    "\n",
    "# used to determine which branch is dominated by another branch\n",
    "function is_dominated(ùí´::POMG, Œ†, i, œÄi)\n",
    "    ‚Ñê, ùíÆ = ùí´.‚Ñê, ùí´.ùíÆ\n",
    "    jointŒ†noti = joint([Œ†[j] for j in ‚Ñê if j ‚â† i])\n",
    "    œÄ(œÄi‚Ä≤, œÄnoti) = [j==i ? œÄi‚Ä≤ : œÄnoti[j>i ? j-1 : j] for j in ‚Ñê]\n",
    "    Ui = Dict((œÄi‚Ä≤, œÄnoti, s) => evaluate_plan(ùí´, œÄ(œÄi‚Ä≤, œÄnoti), s)[i]\n",
    "            for œÄi‚Ä≤ in Œ†[i], œÄnoti in jointŒ†noti, s in ùíÆ)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, Œ¥)\n",
    "    @variable(model, b[jointŒ†noti, ùíÆ] ‚â• 0)\n",
    "    @objective(model, Max, Œ¥)\n",
    "    @constraint(model, [œÄi‚Ä≤=Œ†[i]],\n",
    "        sum(b[œÄnoti, s] * (Ui[œÄi‚Ä≤, œÄnoti, s] - Ui[œÄi, œÄnoti, s])\n",
    "        for œÄnoti in jointŒ†noti for s in ùíÆ) ‚â• Œ¥)\n",
    "    @constraint(model, sum(b) == 1)\n",
    "    optimize!(model)\n",
    "    return value(Œ¥) ‚â• 0\n",
    "end\n",
    "\n",
    "# use to cut branch\n",
    "function prune_dominated!(Œ†, ùí´::POMG)\n",
    "    done = false\n",
    "    while !done\n",
    "        done = true\n",
    "        for i in shuffle(ùí´.‚Ñê)\n",
    "            for œÄi in shuffle(Œ†[i])\n",
    "                if length(Œ†[i]) > 1 && is_dominated(ùí´, Œ†, i, œÄi)\n",
    "                    filter!(œÄi‚Ä≤ -> œÄi‚Ä≤ ‚â† œÄi, Œ†[i])\n",
    "                    done = false\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Dynamic programming computes a Nash equilibrium œÄ for a POMG ùí´, given an initial belief b and horizon depth d. \n",
    "function solve(M::POMGDynamicProgramming, ùí´::POMG)\n",
    "    ‚Ñê, ùíÆ, ùíú, R, Œ≥, b, d = ùí´.‚Ñê, ùí´.ùíÆ, ùí´.ùíú, ùí´.R, ùí´.Œ≥, M.b, M.d\n",
    "    Œ† = [[ConditionalPlan(ai) for ai in ùíú[i]] for i in ‚Ñê]\n",
    "    for t in 1:d\n",
    "        Œ† = expand_conditional_plans(ùí´, Œ†)\n",
    "        prune_dominated!(Œ†, ùí´)\n",
    "    end\n",
    "    ùí¢ = SimpleGame(Œ≥, ‚Ñê, Œ†, œÄ -> utility(ùí´, b, œÄ))\n",
    "    œÄ = solve(NashEquilibrium(), ùí¢)\n",
    "    return Tuple(argmax(œÄi.p) for œÄi in œÄ)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiCaregiver = POMG(0.9, \n",
    "                [1, 2], \n",
    "                [\"HUNGRY\", \"SATED\"], \n",
    "                [[\"FEED\", \"SING\", \"IGNORE\"], [\"FEED\", \"SING\", \"IGNORE\"]], \n",
    "                [[\"CRYING\", \"QUIET\"], [\"CRYING\", \"QUIET\"]], \n",
    "                transition, \n",
    "                joint_observation, \n",
    "                joint_reward);\n",
    "                b = [0.5, 0.5];\n",
    "\n",
    "dyP = POMGDynamicProgramming(b, 1);\n",
    "result = solve(dyP, multiCaregiver);\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8ebd4f208b1081cdcafca2ef9b419109da64a8e17c2878bb5d918614c776af"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
