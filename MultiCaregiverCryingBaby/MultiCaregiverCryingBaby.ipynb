{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Caregiver Crying Baby\n",
    "Partially Observable Markov Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Pkg.installed() is deprecated\n",
      "└ @ Pkg C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\Pkg.jl:595\n",
      "┌ Warning: Pkg.installed() is deprecated\n",
      "└ @ Pkg C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.7\\Pkg\\src\\Pkg.jl:595\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"JuMP\") \n",
    "    Pkg.add(\"JuMP\")\n",
    "end\n",
    "if !haskey(Pkg.installed(), \"Ipopt\")\n",
    "    Pkg.add(\"Ipopt\")\n",
    "end\n",
    "\n",
    "using JuMP, Ipopt, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SING = \"SING\"\n",
    "CRYING = \"CRYING\"\n",
    "QUIET = \"QUIET\"\n",
    "FEED = \"FEED\"\n",
    "SATED = \"SATED\"\n",
    "HUNGRY = \"HUNGRY\"\n",
    "\n",
    "p_cry_when_hungry_in_sing = 0.9\n",
    "p_cry_when_hungry = 0.9\n",
    "p_cry_when_not_hungry = 0.0\n",
    "p_become_hungry = 0.5\n",
    "\n",
    "r_hungry = 10.0\n",
    "r_sing = 0.5\n",
    "r_feed = 5.0\n",
    "\n",
    "struct POMG\n",
    "    γ  # discount factor\n",
    "    ℐ  # agents\n",
    "    𝒮  # state space\n",
    "    𝒜  # joint action space \n",
    "    𝒪  # joint observation space\n",
    "    T  # transition function\n",
    "    O  # joint observation function\n",
    "    R  # joint reward function\n",
    "\n",
    "    function POMG(discount, agents, states, jointAction, jointObservation, transitionFunc, jointObservationFunc, jointRewardFunc)\n",
    "        new(discount, agents, states, jointAction, jointObservation, transitionFunc, jointObservationFunc, jointRewardFunc)\n",
    "    end\n",
    "end\n",
    "\n",
    "struct ConditionalPlan\n",
    "    a   # action to take at root\n",
    "    subplans    # dictionary mapping observations to subplans \n",
    "end\n",
    "\n",
    "struct SimpleGame\n",
    "    γ  # discount factor\n",
    "    ℐ  # agents\n",
    "    𝒜  # joint action space\n",
    "    R  # joint reward function\n",
    "end\n",
    "\n",
    "struct NashEquilibrium end\n",
    "\n",
    "# The general structure of Simple game \n",
    "# source: Algorithms for Decision Making book\n",
    "struct SimpleGamePolicy\n",
    "    p # dictionary mapping actions to probabilities\n",
    "\n",
    "    # Returns a random policy\n",
    "    function SimpleGamePolicy(p::Base.Generator)\n",
    "        return SimpleGamePolicy(Dict(p))\n",
    "    end\n",
    "    # Return policy from dict\n",
    "    function SimpleGamePolicy(p::Dict)\n",
    "        vs = collect(values(p))\n",
    "        vs ./= sum(vs)\n",
    "        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))\n",
    "    end\n",
    "\n",
    "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
    "end\n",
    "\n",
    "ConditionalPlan(a) = ConditionalPlan(a, Dict())\n",
    "\n",
    "(π::ConditionalPlan)() = π.a\n",
    "(π::ConditionalPlan)(o) = π.subplans[o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transition (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function transition(s, a, s′)\n",
    "    # Regardless, feeding makes the baby sated.\n",
    "    if a[1] == \"FEED\" || a[2] == \"FEED\" \n",
    "        if s′ == \"SATED\" \n",
    "            return 1.0\n",
    "        else \n",
    "            return 0.0\n",
    "        end\n",
    "    else\n",
    "        # If neither caretaker feed, then one of two things happens.\n",
    "        # First, a baby that is hungry remains hungry \n",
    "        if s == \"HUNGRY\"\n",
    "            if s′ == \"HUNGRY\"\n",
    "                return 1.0\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        # Otherwise, it becomes hungry with a fixed probability.\n",
    "        else\n",
    "            if s′ == \"SATED\"\n",
    "                return 1.0 - p_become_hungry\n",
    "            else\n",
    "                return p_become_hungry\n",
    "            end \n",
    "        end \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joint_observation (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function joint_observation(a, s′, o)\n",
    "    # If at least one caregiver sings, then both observe the result.\n",
    "    if a[1] == \"SING\" || a[2] == \"SING\"\n",
    "        # If the baby is hungry, then the caregivers both observe crying/silent together.\n",
    "        if s′ == \"HUNGRY\"\n",
    "            if o[1] == \"CRYING\" && o[2] == \"CRYING\"\n",
    "                return p_cry_when_hungry_in_sing\n",
    "            elseif o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0 - p_cry_when_hungry_in_sing\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        # Otherwise the baby is sated\n",
    "        else\n",
    "            if o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        end\n",
    "    # Otherwise the caregivers fed and/or ignored the baby\n",
    "    else \n",
    "        # If the baby is hungry, then there′s a probability it cries\n",
    "        if s′ == \"HUNGRY\"\n",
    "            if o[1] == \"CRYING\" && o[2] == \"CRYING\"\n",
    "                return p_cry_when_hungry \n",
    "            elseif o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0 - p_cry_when_hungry\n",
    "            else \n",
    "                return 0.0\n",
    "            end \n",
    "        # If the baby is sated, then there′s no probability it cries\n",
    "        else\n",
    "            if o[1] == \"CRYING\" && o[2] == \"CRYING\" \n",
    "                return p_cry_when_not_hungry\n",
    "            elseif o[1] == \"QUIET\" && o[2] == \"QUIET\"\n",
    "                return 1.0 - p_cry_when_not_hungry\n",
    "            else \n",
    "                return 0.0\n",
    "            end\n",
    "        end \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joint_reward (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function joint_reward(s, a) \n",
    "    r = [0.0, 0.0]\n",
    "    \n",
    "    # Both caregivers do not want the child to be hungry\n",
    "    if s == \"HUNGRY\"\n",
    "        r -= [r_hungry, r_hungry]\n",
    "    end\n",
    "\n",
    "    # the first caregiver favors feeding \n",
    "    if a[1] == \"FEED\" \n",
    "        r[1] -= r_feed / 2.0 \n",
    "    elseif a[1] == \"SING\"\n",
    "        r[1] -= r_sing\n",
    "    end\n",
    "\n",
    "    # the second caregiver favors singing\n",
    "    if a[2] == \"SING\"\n",
    "        r[2] -= r_sing / 2\n",
    "    elseif a[2] == \"FEED\"\n",
    "        r[2] -= r_feed\n",
    "    end\n",
    "    \n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Conditional Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utility (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The lookahead function below is used to calculate the evaluate plan\n",
    "function lookahead(𝒫::POMG, U, s, a) \n",
    "    𝒮, 𝒪, T, O, R, γ = 𝒫.𝒮, joint(𝒫.𝒪), 𝒫.T, 𝒫.O, 𝒫.R, 𝒫.γ\n",
    "    u′ = sum(T(s,a,s′)*sum(O(a,s′,o)*U(o,s′) for o in 𝒪) for s′ in 𝒮)\n",
    "    return R(s,a) + γ*u′\n",
    "end\n",
    "\n",
    "#  The lookahead function below is used to calculate the utility\n",
    "function evaluate_plan(𝒫::POMG, π, s)\n",
    "    a = Tuple(πi() for πi in π)\n",
    "    U(o,s′) = evaluate_plan(𝒫, [πi(oi) for (πi, oi) in zip(π,o)], s′)\n",
    "    return isempty(first(π).subplans) ? 𝒫.R(s,a) : lookahead(𝒫, U, s, a)\n",
    "end\n",
    "\n",
    "# used to calculate utility with initial belief b when executing joint policy in POMG 𝒫\n",
    "function utility(𝒫::POMG, b, π)\n",
    "    u = [evaluate_plan(𝒫, π, s) for s in 𝒫.𝒮]\n",
    "    return sum(bs * us for (bs, us) in zip(b, u))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash Equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function expand_conditional_plans(𝒫, Π)\n",
    "    ℐ, 𝒜, 𝒪 = 𝒫.ℐ, 𝒫.𝒜, 𝒫.𝒪\n",
    "    return [[ConditionalPlan(ai, Dict(oi => πi for oi in 𝒪[i]))\n",
    "        for πi in Π[i] for ai in 𝒜[i]] for i in ℐ]\n",
    "end\n",
    "\n",
    "joint(X) = vec(collect(Iterators.product(X...)))\n",
    "joint(π, πi, i) = [i == j ? πi : πj for (j, πj) in enumerate(π)]\n",
    "\n",
    "# Returns the format tensor of 𝒫\n",
    "function tensorform(𝒫::SimpleGame)\n",
    "    ℐ, 𝒜, R = 𝒫.ℐ, 𝒫.𝒜, 𝒫.R\n",
    "    ℐ′ = eachindex(ℐ)\n",
    "    𝒜′ = [eachindex(𝒜[i]) for i in ℐ]\n",
    "    R′ = [R(a) for a in joint(𝒜)]\n",
    "    return ℐ′, 𝒜′, R′\n",
    "end\n",
    "\n",
    "# Find the Nash Equilibrium\n",
    "function solve(M::NashEquilibrium, 𝒫::SimpleGame)\n",
    "    ℐ, 𝒜, R = tensorform(𝒫)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    #  declaration\n",
    "    @variable(model, U[ℐ])\n",
    "    # constraint 3\n",
    "    @variable(model, π[i=ℐ, 𝒜[i]] ≥ 0)\n",
    "    # objective function\n",
    "    @NLobjective(model, Min,\n",
    "        sum(U[i] - sum(prod(π[j,a[j]] for j in ℐ) * R[y][i]\n",
    "            for (y,a) in enumerate(joint(𝒜))) for i in ℐ))\n",
    "    # constraint 1\n",
    "    @NLconstraint(model, [i=ℐ, ai=𝒜[i]],\n",
    "        U[i] ≥ sum(\n",
    "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : π[j,a[j]] for j in ℐ)\n",
    "            * R[y][i] for (y,a) in enumerate(joint(𝒜))))\n",
    "    # constrain 2\n",
    "    @constraint(model, [i=ℐ], sum(π[i,ai] for ai in 𝒜[i]) == 1)\n",
    "    # Model optimization\n",
    "    optimize!(model)\n",
    "    πi′(i) = SimpleGamePolicy(𝒫.𝒜[i][ai] => value(π[i,ai]) for ai in 𝒜[i])\n",
    "    return [πi′(i) for i in ℐ]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct POMGDynamicProgramming\n",
    "    b # initial belief\n",
    "    d # depth of conditional plans\n",
    "end\n",
    "\n",
    "# used to determine which branch is dominated by another branch\n",
    "function is_dominated(𝒫::POMG, Π, i, πi)\n",
    "    ℐ, 𝒮 = 𝒫.ℐ, 𝒫.𝒮\n",
    "    jointΠnoti = joint([Π[j] for j in ℐ if j ≠ i])\n",
    "    π(πi′, πnoti) = [j==i ? πi′ : πnoti[j>i ? j-1 : j] for j in ℐ]\n",
    "    Ui = Dict((πi′, πnoti, s) => evaluate_plan(𝒫, π(πi′, πnoti), s)[i]\n",
    "            for πi′ in Π[i], πnoti in jointΠnoti, s in 𝒮)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, δ)\n",
    "    @variable(model, b[jointΠnoti, 𝒮] ≥ 0)\n",
    "    @objective(model, Max, δ)\n",
    "    @constraint(model, [πi′=Π[i]],\n",
    "        sum(b[πnoti, s] * (Ui[πi′, πnoti, s] - Ui[πi, πnoti, s])\n",
    "        for πnoti in jointΠnoti for s in 𝒮) ≥ δ)\n",
    "    @constraint(model, sum(b) == 1)\n",
    "    optimize!(model)\n",
    "    return value(δ) ≥ 0\n",
    "end\n",
    "\n",
    "# use to cut branch\n",
    "function prune_dominated!(Π, 𝒫::POMG)\n",
    "    done = false\n",
    "    while !done\n",
    "        done = true\n",
    "        for i in shuffle(𝒫.ℐ)\n",
    "            for πi in shuffle(Π[i])\n",
    "                if length(Π[i]) > 1 && is_dominated(𝒫, Π, i, πi)\n",
    "                    filter!(πi′ -> πi′ ≠ πi, Π[i])\n",
    "                    done = false\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Dynamic programming computes a Nash equilibrium π for a POMG 𝒫, given an initial belief b and horizon depth d. \n",
    "function solve(M::POMGDynamicProgramming, 𝒫::POMG)\n",
    "    ℐ, 𝒮, 𝒜, R, γ, b, d = 𝒫.ℐ, 𝒫.𝒮, 𝒫.𝒜, 𝒫.R, 𝒫.γ, M.b, M.d\n",
    "    Π = [[ConditionalPlan(ai) for ai in 𝒜[i]] for i in ℐ]\n",
    "    for t in 1:d\n",
    "        Π = expand_conditional_plans(𝒫, Π)\n",
    "        prune_dominated!(Π, 𝒫)\n",
    "    end\n",
    "    𝒢 = SimpleGame(γ, ℐ, Π, π -> utility(𝒫, b, π))\n",
    "    π = solve(NashEquilibrium(), 𝒢)\n",
    "    return Tuple(argmax(πi.p) for πi in π)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiCaregiver = POMG(0.9, \n",
    "                [1, 2], \n",
    "                [\"HUNGRY\", \"SATED\"], \n",
    "                [[\"FEED\", \"SING\", \"IGNORE\"], [\"FEED\", \"SING\", \"IGNORE\"]], \n",
    "                [[\"CRYING\", \"QUIET\"], [\"CRYING\", \"QUIET\"]], \n",
    "                transition, \n",
    "                joint_observation, \n",
    "                joint_reward);\n",
    "                b = [0.5, 0.5];\n",
    "\n",
    "dyP = POMGDynamicProgramming(b, 1);\n",
    "result = solve(dyP, multiCaregiver);\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8ebd4f208b1081cdcafca2ef9b419109da64a8e17c2878bb5d918614c776af"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
