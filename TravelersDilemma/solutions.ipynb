{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f172b7e6",
   "metadata": {},
   "source": [
    "# Traveler's Dilemma\n",
    "Simple Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a5d11",
   "metadata": {},
   "source": [
    "*Ghi chú:* **Các giải thuật dưới đây là những lời giải tốt nhất tìm được để chứng minh cho kết luận của bài toán. Các giải thuật khác đã thử nghiệm và kết quả của chúng được ghi trong báo cáo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd87c8",
   "metadata": {},
   "source": [
    "## Resource\n",
    "Add the necessary packages and declare some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad34be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "\n",
    "function addPackage(pkg::String)\n",
    "    if !haskey(Pkg.installed(), pkg)\n",
    "        Pkg.add(pkg)\n",
    "    end\n",
    "end\n",
    "\n",
    "addPackage(\"Distributions\")\n",
    "addPackage(\"LinearAlgebra\")\n",
    "addPackage(\"JuMP\")\n",
    "addPackage(\"Ipopt\")\n",
    "\n",
    "using Distributions, LinearAlgebra, JuMP, Ipopt, Random\n",
    "\n",
    "\n",
    "# Appendices\n",
    "# G.5 Convenience Functions\n",
    "struct SetCategorical{S}\n",
    "    elements::Vector{S} # Set elements (could be repeated)\n",
    "    distr::Categorical # Categorical distribution over set elements\n",
    "\n",
    "    function SetCategorical(elements::AbstractVector{S}) where {S}\n",
    "        weights = ones(length(elements))\n",
    "        return new{S}(elements, Categorical(normalize(weights, 1)))\n",
    "    end\n",
    "\n",
    "    function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where {S}\n",
    "        ℓ₁ = norm(weights, 1)\n",
    "        if ℓ₁ < 1e-6 || isinf(ℓ₁)\n",
    "            return SetCategorical(elements)\n",
    "        end\n",
    "        distr = Categorical(normalize(weights, 1))\n",
    "        return new{S}(elements, distr)\n",
    "    end\n",
    "end\n",
    "\n",
    "Distributions.rand(D::SetCategorical) = D.elements[rand(D.distr)]\n",
    "Distributions.rand(D::SetCategorical, n::Int) = D.elements[rand(D.distr, n)]\n",
    "\n",
    "function Distributions.pdf(D::SetCategorical, x)\n",
    "    sum(e == x ? w : 0.0 for (e, w) in zip(D.elements, D.distr.p))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580af32e",
   "metadata": {},
   "source": [
    "## Define simple game structures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f6649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Algorithm 24.1. Data structure for a simple game.\n",
    "struct SimpleGame\n",
    "    γ # discount factor\n",
    "    ℐ # agents\n",
    "    𝒜 # joint action space\n",
    "    R # joint reward function\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.2\n",
    "struct SimpleGamePolicy\n",
    "    p # dictionary mapping actions to probabilities\n",
    "\n",
    "    function SimpleGamePolicy(p::Base.Generator)\n",
    "        return SimpleGamePolicy(Dict(p))\n",
    "    end\n",
    "\n",
    "    function SimpleGamePolicy(p::Dict)\n",
    "        vs = collect(values(p))\n",
    "        vs ./= sum(vs)\n",
    "        return new(Dict(k => v for (k, v) in zip(keys(p), vs)))\n",
    "    end\n",
    "\n",
    "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
    "end\n",
    "\n",
    "(πi::SimpleGamePolicy)(ai) = get(πi.p, ai, 0.0)\n",
    "\n",
    "function (πi::SimpleGamePolicy)()\n",
    "    D = SetCategorical(collect(keys(πi.p)), collect(values(πi.p)))\n",
    "    return rand(D)\n",
    "end\n",
    "\n",
    "joint(X) = vec(collect(Iterators.product(X...)))\n",
    "\n",
    "joint(π, πi, i) = [i == j ? πi : πj for (j, πj) in enumerate(π)] # helper of best_response\n",
    "\n",
    "function utility(𝒫::SimpleGame, π, i)\n",
    "    𝒜, R = 𝒫.𝒜, 𝒫.R\n",
    "    p(a) = prod(πj(aj) for (πj, aj) in zip(π, a))\n",
    "    return sum(R(a)[i] * p(a) for a in joint(𝒜))\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.3\n",
    "function best_response(𝒫::SimpleGame, π, i)\n",
    "    U(ai) = utility(𝒫, joint(π, SimpleGamePolicy(ai), i), i)\n",
    "    ai = argmax(U, 𝒫.𝒜[i])\n",
    "    return SimpleGamePolicy(ai)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0d7fb",
   "metadata": {},
   "source": [
    "## Problem properties\n",
    "Register the properties of Traveler's Dilemma problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "const N_AGENTS = 2\n",
    "ACTIONS = vec(collect(2:100))\n",
    "\n",
    "function joint_reward(a::Tuple{Int64,Int64})\n",
    "    ai, aj = a\n",
    "    return ai == aj ? (ai, aj) : (ai < aj ? (ai + 2, ai - 2) : (aj - 2, aj + 2))\n",
    "end\n",
    "\n",
    "travelersDilemma = SimpleGame(\n",
    "    1.0,\n",
    "    vec(collect(1:N_AGENTS)),\n",
    "    [ACTIONS for _ in 1:N_AGENTS],\n",
    "    joint_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511468a",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 24.4\n",
    "function softmax_response(𝒫::SimpleGame, π, i, λ)\n",
    "    𝒜i = 𝒫.𝒜[i]\n",
    "    U(ai) = utility(𝒫, joint(π, SimpleGamePolicy(ai), i), i)\n",
    "    return SimpleGamePolicy(ai => exp(λ * U(ai)) for ai in 𝒜i)\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.9\n",
    "struct HierarchicalSoftmax\n",
    "    λ # precision parameter\n",
    "    k # level\n",
    "    π # initial policy\n",
    "end\n",
    "\n",
    "function HierarchicalSoftmax(𝒫::SimpleGame, λ, k)\n",
    "    π = [SimpleGamePolicy(ai => 1.0 for ai in 𝒜i) for 𝒜i in 𝒫.𝒜]\n",
    "    return HierarchicalSoftmax(λ, k, π)\n",
    "end\n",
    "\n",
    "function solve(M::HierarchicalSoftmax, 𝒫)\n",
    "    π = M.π\n",
    "    for k in 1:M.k\n",
    "        π = [softmax_response(𝒫, π, i, M.λ) for i in 𝒫.ℐ]\n",
    "    end\n",
    "    return π\n",
    "end\n",
    "\n",
    "\n",
    "π = solve(HierarchicalSoftmax(travelersDilemma, 0.3, 4), travelersDilemma)\n",
    "\n",
    "π¹ = π[1].p\n",
    "π² = π[2].p\n",
    "\n",
    "for a in ACTIONS\n",
    "    println(a => (π¹[a], π²[a]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0218a",
   "metadata": {},
   "source": [
    "## Reduce the number of actions available\n",
    "Reduce the number of actions available from \\\\$100 to \\\\$20 maximum value can be put down for saving runtime. It is easy to see that this does not greatly affect the conclusion of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = vec(collect(2:20))\n",
    "\n",
    "travelersDilemma = SimpleGame(\n",
    "    1.0,\n",
    "    vec(collect(1:N_AGENTS)),\n",
    "    [ACTIONS for _ in 1:N_AGENTS],\n",
    "    joint_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57a294",
   "metadata": {},
   "source": [
    "## Correlated Equilibrium solution\n",
    "General of Nash equilibrium concept by relaxing the assumption that the agents act independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d117c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Algorithm 24.6\n",
    "mutable struct JointCorrelatedPolicy\n",
    "    p # dictionary mapping from joint actions to probabilities\n",
    "    JointCorrelatedPolicy(p::Base.Generator) = new(Dict(p))\n",
    "end\n",
    "\n",
    "(π::JointCorrelatedPolicy)(a) = get(π.p, a, 0.0)\n",
    "\n",
    "function (π::JointCorrelatedPolicy)()\n",
    "    D = SetCategorical(collect(keys(π.p)), collect(values(π.p)))\n",
    "    return rand(D)\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.7 (Utilitarian) [Fixed bug by me]\n",
    "struct CorrelatedEquilibrium end\n",
    "\n",
    "joint(a, ai′, i) = Tuple(k == i ? ai′ : v for (k, v) in enumerate(a))\n",
    "\n",
    "function solve(M::CorrelatedEquilibrium, 𝒫::SimpleGame)\n",
    "    ℐ, 𝒜, R = 𝒫.ℐ, 𝒫.𝒜, 𝒫.R\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, π[joint(𝒜)] ≥ 0)\n",
    "    @objective(model, Max, sum(sum(π[a] * R(a)[i] for a in joint(𝒜)) for i in ℐ))\n",
    "    @constraint(model, [i = ℐ, ai = 𝒜[i], ai′ = 𝒜[i]],\n",
    "        sum(R(a)[i] * π[a] for a in joint(𝒜) if a[i] == ai)\n",
    "        ≥\n",
    "        sum(R(joint(a, ai′, i))[i] * π[a] for a in joint(𝒜) if a[i] == ai))\n",
    "    @constraint(model, sum(π) == 1)\n",
    "    optimize!(model)\n",
    "    return JointCorrelatedPolicy(a => value(π[a]) for a in joint(𝒜))\n",
    "end\n",
    "\n",
    "\n",
    "π = solve(CorrelatedEquilibrium(), travelersDilemma)\n",
    "\n",
    "π¹ = Dict(a => 0.0 for a in travelersDilemma.𝒜[1])\n",
    "π² = Dict(a => 0.0 for a in travelersDilemma.𝒜[2])\n",
    "\n",
    "for (k, v) in π.p\n",
    "    π¹[k[1]] += v\n",
    "    π²[k[2]] += v\n",
    "end\n",
    "\n",
    "for a in ACTIONS\n",
    "    println(a => (π¹[a], π²[a]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ff90e",
   "metadata": {},
   "source": [
    "## Fictitious Play solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 24.11\n",
    "mutable struct FictitiousPlay\n",
    "    𝒫 # simple game\n",
    "    i # agent index\n",
    "    N # array of action count dictionaries\n",
    "    πi # current policy\n",
    "end\n",
    "\n",
    "function FictitiousPlay(𝒫::SimpleGame, i)\n",
    "    N = [Dict(aj => 1 for aj in 𝒫.𝒜[j]) for j in 𝒫.ℐ]\n",
    "    πi = SimpleGamePolicy(ai => 1.0 for ai in 𝒫.𝒜[i])\n",
    "    return FictitiousPlay(𝒫, i, N, πi)\n",
    "end\n",
    "\n",
    "(πi::FictitiousPlay)() = πi.πi()\n",
    "\n",
    "(πi::FictitiousPlay)(ai) = πi.πi(ai)\n",
    "\n",
    "function update!(πi::FictitiousPlay, a)\n",
    "    N, 𝒫, ℐ, i = πi.N, πi.𝒫, πi.𝒫.ℐ, πi.i\n",
    "    for (j, aj) in enumerate(a)\n",
    "        N[j][aj] += 1\n",
    "    end\n",
    "    p(j) = SimpleGamePolicy(aj => u / sum(values(N[j])) for (aj, u) in N[j])\n",
    "    π = [p(j) for j in ℐ]\n",
    "    πi.πi = best_response(𝒫, π, i)\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.10\n",
    "function simulate(𝒫::SimpleGame, π, k_max)\n",
    "    for k = 1:k_max\n",
    "        a = [πi() for πi in π]\n",
    "        for πi in π\n",
    "            update!(πi, a)\n",
    "        end\n",
    "    end\n",
    "    return π\n",
    "end\n",
    "\n",
    "\n",
    "for k_max in [100, 1000, 10000, 100000]\n",
    "    π = simulate(\n",
    "        travelersDilemma,\n",
    "        [FictitiousPlay(travelersDilemma, i) for i in travelersDilemma.ℐ],\n",
    "        k_max)\n",
    "\n",
    "    println(\"After \", k_max, \" iterations, the (deterministic) policy:\")\n",
    "    \n",
    "    π¹ = π[1].πi\n",
    "    π² = π[2].πi\n",
    "    \n",
    "    println(\"π¹ = \", π¹)\n",
    "    println(\"π² = \", π²)\n",
    "    println()\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
