{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f172b7e6",
   "metadata": {},
   "source": [
    "# Traveler's Dilemma\n",
    "Simple Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a5d11",
   "metadata": {},
   "source": [
    "*Ghi chÃº:* **CÃ¡c giáº£i thuáº­t dÆ°á»›i Ä‘Ã¢y lÃ  nhá»¯ng lá»i giáº£i tá»‘t nháº¥t tÃ¬m Ä‘Æ°á»£c Ä‘á»ƒ chá»©ng minh cho káº¿t luáº­n cá»§a bÃ i toÃ¡n. CÃ¡c giáº£i thuáº­t khÃ¡c Ä‘Ã£ thá»­ nghiá»‡m vÃ  káº¿t quáº£ cá»§a chÃºng Ä‘Æ°á»£c ghi trong bÃ¡o cÃ¡o.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd87c8",
   "metadata": {},
   "source": [
    "## Resource\n",
    "Add the necessary packages and declare some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad34be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "\n",
    "function addPackage(pkg::String)\n",
    "    if !haskey(Pkg.installed(), pkg)\n",
    "        Pkg.add(pkg)\n",
    "    end\n",
    "end\n",
    "\n",
    "addPackage(\"Distributions\")\n",
    "addPackage(\"LinearAlgebra\")\n",
    "addPackage(\"JuMP\")\n",
    "addPackage(\"Ipopt\")\n",
    "\n",
    "using Distributions, LinearAlgebra, JuMP, Ipopt, Random\n",
    "\n",
    "\n",
    "# Appendices\n",
    "# G.5 Convenience Functions\n",
    "struct SetCategorical{S}\n",
    "    elements::Vector{S} # Set elements (could be repeated)\n",
    "    distr::Categorical # Categorical distribution over set elements\n",
    "\n",
    "    function SetCategorical(elements::AbstractVector{S}) where {S}\n",
    "        weights = ones(length(elements))\n",
    "        return new{S}(elements, Categorical(normalize(weights, 1)))\n",
    "    end\n",
    "\n",
    "    function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where {S}\n",
    "        â„“â‚ = norm(weights, 1)\n",
    "        if â„“â‚ < 1e-6 || isinf(â„“â‚)\n",
    "            return SetCategorical(elements)\n",
    "        end\n",
    "        distr = Categorical(normalize(weights, 1))\n",
    "        return new{S}(elements, distr)\n",
    "    end\n",
    "end\n",
    "\n",
    "Distributions.rand(D::SetCategorical) = D.elements[rand(D.distr)]\n",
    "Distributions.rand(D::SetCategorical, n::Int) = D.elements[rand(D.distr, n)]\n",
    "\n",
    "function Distributions.pdf(D::SetCategorical, x)\n",
    "    sum(e == x ? w : 0.0 for (e, w) in zip(D.elements, D.distr.p))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580af32e",
   "metadata": {},
   "source": [
    "## Define simple game structures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f6649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Algorithm 24.1. Data structure for a simple game.\n",
    "struct SimpleGame\n",
    "    Î³ # discount factor\n",
    "    â„ # agents\n",
    "    ğ’œ # joint action space\n",
    "    R # joint reward function\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.2\n",
    "struct SimpleGamePolicy\n",
    "    p # dictionary mapping actions to probabilities\n",
    "\n",
    "    function SimpleGamePolicy(p::Base.Generator)\n",
    "        return SimpleGamePolicy(Dict(p))\n",
    "    end\n",
    "\n",
    "    function SimpleGamePolicy(p::Dict)\n",
    "        vs = collect(values(p))\n",
    "        vs ./= sum(vs)\n",
    "        return new(Dict(k => v for (k, v) in zip(keys(p), vs)))\n",
    "    end\n",
    "\n",
    "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
    "end\n",
    "\n",
    "(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)\n",
    "\n",
    "function (Ï€i::SimpleGamePolicy)()\n",
    "    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))\n",
    "    return rand(D)\n",
    "end\n",
    "\n",
    "joint(X) = vec(collect(Iterators.product(X...)))\n",
    "\n",
    "joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)] # helper of best_response\n",
    "\n",
    "function utility(ğ’«::SimpleGame, Ï€, i)\n",
    "    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R\n",
    "    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))\n",
    "    return sum(R(a)[i] * p(a) for a in joint(ğ’œ))\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.3\n",
    "function best_response(ğ’«::SimpleGame, Ï€, i)\n",
    "    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)\n",
    "    ai = argmax(U, ğ’«.ğ’œ[i])\n",
    "    return SimpleGamePolicy(ai)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0d7fb",
   "metadata": {},
   "source": [
    "## Problem properties\n",
    "Register the properties of Traveler's Dilemma problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "const N_AGENTS = 2\n",
    "ACTIONS = vec(collect(2:100))\n",
    "\n",
    "function joint_reward(a::Tuple{Int64,Int64})\n",
    "    ai, aj = a\n",
    "    return ai == aj ? (ai, aj) : (ai < aj ? (ai + 2, ai - 2) : (aj - 2, aj + 2))\n",
    "end\n",
    "\n",
    "travelersDilemma = SimpleGame(\n",
    "    1.0,\n",
    "    vec(collect(1:N_AGENTS)),\n",
    "    [ACTIONS for _ in 1:N_AGENTS],\n",
    "    joint_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511468a",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 24.4\n",
    "function softmax_response(ğ’«::SimpleGame, Ï€, i, Î»)\n",
    "    ğ’œi = ğ’«.ğ’œ[i]\n",
    "    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)\n",
    "    return SimpleGamePolicy(ai => exp(Î» * U(ai)) for ai in ğ’œi)\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.9\n",
    "struct HierarchicalSoftmax\n",
    "    Î» # precision parameter\n",
    "    k # level\n",
    "    Ï€ # initial policy\n",
    "end\n",
    "\n",
    "function HierarchicalSoftmax(ğ’«::SimpleGame, Î», k)\n",
    "    Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]\n",
    "    return HierarchicalSoftmax(Î», k, Ï€)\n",
    "end\n",
    "\n",
    "function solve(M::HierarchicalSoftmax, ğ’«)\n",
    "    Ï€ = M.Ï€\n",
    "    for k in 1:M.k\n",
    "        Ï€ = [softmax_response(ğ’«, Ï€, i, M.Î») for i in ğ’«.â„]\n",
    "    end\n",
    "    return Ï€\n",
    "end\n",
    "\n",
    "\n",
    "Ï€ = solve(HierarchicalSoftmax(travelersDilemma, 0.3, 4), travelersDilemma)\n",
    "\n",
    "Ï€Â¹ = Ï€[1].p\n",
    "Ï€Â² = Ï€[2].p\n",
    "\n",
    "for a in ACTIONS\n",
    "    println(a => (Ï€Â¹[a], Ï€Â²[a]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0218a",
   "metadata": {},
   "source": [
    "## Reduce the number of actions available\n",
    "Reduce the number of actions available from \\\\$100 to \\\\$20 maximum value can be put down for saving runtime. It is easy to see that this does not greatly affect the conclusion of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = vec(collect(2:20))\n",
    "\n",
    "travelersDilemma = SimpleGame(\n",
    "    1.0,\n",
    "    vec(collect(1:N_AGENTS)),\n",
    "    [ACTIONS for _ in 1:N_AGENTS],\n",
    "    joint_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57a294",
   "metadata": {},
   "source": [
    "## Correlated Equilibrium solution\n",
    "General of Nash equilibrium concept by relaxing the assumption that the agents act independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d117c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Algorithm 24.6\n",
    "mutable struct JointCorrelatedPolicy\n",
    "    p # dictionary mapping from joint actions to probabilities\n",
    "    JointCorrelatedPolicy(p::Base.Generator) = new(Dict(p))\n",
    "end\n",
    "\n",
    "(Ï€::JointCorrelatedPolicy)(a) = get(Ï€.p, a, 0.0)\n",
    "\n",
    "function (Ï€::JointCorrelatedPolicy)()\n",
    "    D = SetCategorical(collect(keys(Ï€.p)), collect(values(Ï€.p)))\n",
    "    return rand(D)\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.7 (Utilitarian) [Fixed bug by me]\n",
    "struct CorrelatedEquilibrium end\n",
    "\n",
    "joint(a, aiâ€², i) = Tuple(k == i ? aiâ€² : v for (k, v) in enumerate(a))\n",
    "\n",
    "function solve(M::CorrelatedEquilibrium, ğ’«::SimpleGame)\n",
    "    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, Ï€[joint(ğ’œ)] â‰¥ 0)\n",
    "    @objective(model, Max, sum(sum(Ï€[a] * R(a)[i] for a in joint(ğ’œ)) for i in â„))\n",
    "    @constraint(model, [i = â„, ai = ğ’œ[i], aiâ€² = ğ’œ[i]],\n",
    "        sum(R(a)[i] * Ï€[a] for a in joint(ğ’œ) if a[i] == ai)\n",
    "        â‰¥\n",
    "        sum(R(joint(a, aiâ€², i))[i] * Ï€[a] for a in joint(ğ’œ) if a[i] == ai))\n",
    "    @constraint(model, sum(Ï€) == 1)\n",
    "    optimize!(model)\n",
    "    return JointCorrelatedPolicy(a => value(Ï€[a]) for a in joint(ğ’œ))\n",
    "end\n",
    "\n",
    "\n",
    "Ï€ = solve(CorrelatedEquilibrium(), travelersDilemma)\n",
    "\n",
    "Ï€Â¹ = Dict(a => 0.0 for a in travelersDilemma.ğ’œ[1])\n",
    "Ï€Â² = Dict(a => 0.0 for a in travelersDilemma.ğ’œ[2])\n",
    "\n",
    "for (k, v) in Ï€.p\n",
    "    Ï€Â¹[k[1]] += v\n",
    "    Ï€Â²[k[2]] += v\n",
    "end\n",
    "\n",
    "for a in ACTIONS\n",
    "    println(a => (Ï€Â¹[a], Ï€Â²[a]))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ff90e",
   "metadata": {},
   "source": [
    "## Fictitious Play solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf168bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 24.11\n",
    "mutable struct FictitiousPlay\n",
    "    ğ’« # simple game\n",
    "    i # agent index\n",
    "    N # array of action count dictionaries\n",
    "    Ï€i # current policy\n",
    "end\n",
    "\n",
    "function FictitiousPlay(ğ’«::SimpleGame, i)\n",
    "    N = [Dict(aj => 1 for aj in ğ’«.ğ’œ[j]) for j in ğ’«.â„]\n",
    "    Ï€i = SimpleGamePolicy(ai => 1.0 for ai in ğ’«.ğ’œ[i])\n",
    "    return FictitiousPlay(ğ’«, i, N, Ï€i)\n",
    "end\n",
    "\n",
    "(Ï€i::FictitiousPlay)() = Ï€i.Ï€i()\n",
    "\n",
    "(Ï€i::FictitiousPlay)(ai) = Ï€i.Ï€i(ai)\n",
    "\n",
    "function update!(Ï€i::FictitiousPlay, a)\n",
    "    N, ğ’«, â„, i = Ï€i.N, Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.i\n",
    "    for (j, aj) in enumerate(a)\n",
    "        N[j][aj] += 1\n",
    "    end\n",
    "    p(j) = SimpleGamePolicy(aj => u / sum(values(N[j])) for (aj, u) in N[j])\n",
    "    Ï€ = [p(j) for j in â„]\n",
    "    Ï€i.Ï€i = best_response(ğ’«, Ï€, i)\n",
    "end\n",
    "\n",
    "\n",
    "# Algorithm 24.10\n",
    "function simulate(ğ’«::SimpleGame, Ï€, k_max)\n",
    "    for k = 1:k_max\n",
    "        a = [Ï€i() for Ï€i in Ï€]\n",
    "        for Ï€i in Ï€\n",
    "            update!(Ï€i, a)\n",
    "        end\n",
    "    end\n",
    "    return Ï€\n",
    "end\n",
    "\n",
    "\n",
    "for k_max in [100, 1000, 10000, 100000]\n",
    "    Ï€ = simulate(\n",
    "        travelersDilemma,\n",
    "        [FictitiousPlay(travelersDilemma, i) for i in travelersDilemma.â„],\n",
    "        k_max)\n",
    "\n",
    "    println(\"After \", k_max, \" iterations, the (deterministic) policy:\")\n",
    "    \n",
    "    Ï€Â¹ = Ï€[1].Ï€i\n",
    "    Ï€Â² = Ï€[2].Ï€i\n",
    "    \n",
    "    println(\"Ï€Â¹ = \", Ï€Â¹)\n",
    "    println(\"Ï€Â² = \", Ï€Â²)\n",
    "    println()\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
