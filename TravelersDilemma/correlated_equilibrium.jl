import Pkg

function addPackage(pkg::String)
    if haskey(Pkg.dependencies(), pkg)
        Pkg.add(pkg)
    end
end

addPackage("Distributions")
addPackage("LinearAlgebra")
addPackage("JuMP")
addPackage("Ipopt")

using Distributions, LinearAlgebra, JuMP, Ipopt, Random


# Appendices
# G.5 Convenience Functions
struct SetCategorical{S}
    elements::Vector{S} # Set elements (could be repeated)
    distr::Categorical # Categorical distribution over set elements

    function SetCategorical(elements::AbstractVector{S}) where {S}
        weights = ones(length(elements))
        return new{S}(elements, Categorical(normalize(weights, 1)))
    end

    function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where {S}
        â„“â‚ = norm(weights, 1)
        if â„“â‚ < 1e-6 || isinf(â„“â‚)
            return SetCategorical(elements)
        end
        distr = Categorical(normalize(weights, 1))
        return new{S}(elements, distr)
    end
end

Distributions.rand(D::SetCategorical) = D.elements[rand(D.distr)]
Distributions.rand(D::SetCategorical, n::Int) = D.elements[rand(D.distr, n)]

function Distributions.pdf(D::SetCategorical, x)
    sum(e == x ? w : 0.0 for (e, w) in zip(D.elements, D.distr.p))
end



# Algorithm 24.1. Data structure for a simple game.
struct SimpleGame
    Î³ # discount factor
    â„ # agents
    ğ’œ # joint action space
    R # joint reward function
end


# Algorithm 24.2
struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities

    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k, v) in zip(keys(p), vs)))
    end

    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end

(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)

function (Ï€i::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
    return rand(D)
end

joint(X) = vec(collect(Iterators.product(X...)))

joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]

function utility(ğ’«::SimpleGame, Ï€, i)
    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
    return sum(R(a)[i] * p(a) for a in joint(ğ’œ))
end


const N_AGENTS = 2
const ACTIONS = vec(collect(2:100))

function joint_reward(a::Tuple{Int64, Int64})
    ai, aj = a
    if ai == aj
        return (ai, ai)
    elseif ai < aj
        return (ai + 2, ai - 2)
    end
    return (aj - 2, aj + 2)
end

travelersDilemma = SimpleGame(
    1.0,
    vec(collect(1:N_AGENTS)),
    [ACTIONS for _ in 1:N_AGENTS],
    joint_reward)


# Algorithm 24.6
mutable struct JointCorrelatedPolicy
    p # dictionary mapping from joint actions to probabilities
    JointCorrelatedPolicy(p::Base.Generator) = new(Dict(p))
end

(Ï€::JointCorrelatedPolicy)(a) = get(Ï€.p, a, 0.0)

function (Ï€::JointCorrelatedPolicy)()
    D = SetCategorical(collect(keys(Ï€.p)), collect(values(Ï€.p)))
    return rand(D)
end


# Algorithm 24.7
struct CorrelatedEquilibrium end

function solve(M::CorrelatedEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    model = Model(Ipopt.Optimizer)
    @variable(model, Ï€[joint(ğ’œ)] â‰¥ 0)
    @objective(model, Max, sum(sum(Ï€[a] * R(a) for a in joint(ğ’œ))))
    @constraint(model, [i = â„, ai = ğ’œ[i], aiâ€² = ğ’œ[i]],
        sum(R(a)[i] * Ï€[a] for a in joint(ğ’œ) if a[i] == ai)
        â‰¥
        sum(R(joint(a, aiâ€², i))[i] * Ï€[a] for a in joint(ğ’œ) if a[i] == ai))
    @constraint(model, sum(Ï€) == 1)
    optimize!(model)
    return JointCorrelatedPolicy(a => value(Ï€[a]) for a in joint(ğ’œ))
end


Ï€ = solve(CorrelatedEquilibrium(), travelersDilemma)