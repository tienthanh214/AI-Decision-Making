{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rock-Paper-Scissors\n",
    "\n",
    "Simple Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "if !haskey(Pkg.dependencies(), \"Ipopt\")\n",
    "    Pkg.add(\"Ipopt\")\n",
    "end\n",
    "if !haskey(Pkg.dependencies(), \"JuMP\")\n",
    "    Pkg.add(\"JuMP\")\n",
    "end\n",
    "\n",
    "using JuMP, Ipopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct Simple Game\n",
    "\n",
    "(Source: Algorithms for decision making)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct SimpleGame\n",
    "    γ  # discount factor\n",
    "    ℐ  # agents\n",
    "    𝒜  # joint action space\n",
    "    R  # joint reward function\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct Simple Game Policy and utility calculate function\n",
    "\n",
    "(Source: Algorithms for decision making, modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utility (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "struct SimpleGamePolicy\n",
    "    p # dictionary mapping actions to probabilities\n",
    "\n",
    "    function SimpleGamePolicy(p::Base.Generator)\n",
    "        return SimpleGamePolicy(Dict(p))\n",
    "    end\n",
    "\n",
    "    function SimpleGamePolicy(p::Dict)\n",
    "        vs = collect(values(p))\n",
    "        vs ./= sum(vs)\n",
    "        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))\n",
    "    end\n",
    "\n",
    "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
    "end\n",
    "\n",
    "# get probability that policy πi choose action ai\n",
    "(πi::SimpleGamePolicy)(ai) = get(πi.p, ai, 0.0)\n",
    "\n",
    "# get a random action according to policy πi\n",
    "function (πi::SimpleGamePolicy)()\n",
    "    actions = vec(collect(keys(πi.p)))\n",
    "    probs = vec(collect(values(πi.p)))\n",
    "    return actions[findfirst(cumsum(probs) .> rand())]\n",
    "end\n",
    "\n",
    "# construct joint space\n",
    "joint(X) = vec(collect(Iterators.product(X...)))\n",
    "\n",
    "# function to calculate utility of agent i when playing simple game 𝒫 with joint policy π\n",
    "function utility(𝒫::SimpleGame, π, i)\n",
    "    𝒜, R = 𝒫.𝒜, 𝒫.R\n",
    "    p(a) = prod(πj(aj) for (πj, aj) in zip(π, a))\n",
    "    return sum(R[a][i]*p(a) for a in joint(𝒜))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rock-Paper-Scissors game\n",
    "\n",
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Symbol}:\n",
       " :rock\n",
       " :paper\n",
       " :scissors"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const N_AGENTS = 2\n",
    "const ACTIONS = [:rock, :paper, :scissors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "struct RockPaperScissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct RockPaperScissors\n",
    "    simpleGame::SimpleGame\n",
    "\n",
    "    function RockPaperScissors(\n",
    "        γ::Real,                        # discount factor\n",
    "        rewards::Dict{Symbol, Real},    # reward if winning by select an action, opponent's losing reward is negated\n",
    "        )\n",
    "\n",
    "        # construct joint rewards\n",
    "        joint_rewards = construct_joint_rewards(rewards)\n",
    "\n",
    "        simpleGame = SimpleGame(γ, vec(collect(1:N_AGENTS)), [ACTIONS for _ in 1:N_AGENTS], joint_rewards)\n",
    "\n",
    "        return new(simpleGame)\n",
    "    end\n",
    "\n",
    "    function construct_joint_rewards(rewards::Dict{Symbol, Real})\n",
    "        joint_rewards = Dict{Tuple{Symbol, Symbol}, Tuple{Real, Real}}()\n",
    "\n",
    "        # tie\n",
    "        joint_rewards[(:rock, :rock)] = joint_rewards[(:paper, :paper)] = joint_rewards[(:scissors, :scissors)] = (0, 0)\n",
    "\n",
    "        # rock beats scisssors\n",
    "        joint_rewards[(:rock, :scissors)] = (rewards[:rock], -rewards[:rock])\n",
    "        joint_rewards[(:scissors, :rock)] = (-rewards[:rock], rewards[:rock])\n",
    "\n",
    "        # scisssors beats paper\n",
    "        joint_rewards[(:scissors, :paper)] = (rewards[:scissors], -rewards[:scissors])\n",
    "        joint_rewards[(:paper, :scissors)] = (-rewards[:scissors], rewards[:scissors])\n",
    "\n",
    "        # paper beats rock\n",
    "        joint_rewards[(:paper, :rock)] = (rewards[:paper], -rewards[:paper])\n",
    "        joint_rewards[(:rock, :paper)] = (-rewards[:paper], rewards[:paper])\n",
    "\n",
    "        return joint_rewards\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash Equilibrium\n",
    "\n",
    "(Source: Algorithms for decision making)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "struct NashEquilibrium end\n",
    "\n",
    "function tensorform(𝒫::SimpleGame)\n",
    "    ℐ, 𝒜, R = 𝒫.ℐ, 𝒫.𝒜, 𝒫.R\n",
    "    ℐ′ = eachindex(ℐ)\n",
    "    𝒜′ = [eachindex(𝒜[i]) for i in ℐ]\n",
    "    R′ = [R[a] for a in joint(𝒜)]\n",
    "    return ℐ′, 𝒜′, R′\n",
    "end\n",
    "\n",
    "function solve(M::NashEquilibrium, 𝒫::SimpleGame)\n",
    "    ℐ, 𝒜, R = tensorform(𝒫)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, U[ℐ])\n",
    "    @variable(model, π[i=ℐ, 𝒜[i]] ≥ 0)\n",
    "    @NLobjective(model, Min,\n",
    "        sum(U[i] - sum(prod(π[j,a[j]] for j in ℐ) * R[y][i]\n",
    "            for (y,a) in enumerate(joint(𝒜))) for i in ℐ))\n",
    "    @NLconstraint(model, [i=ℐ, ai=𝒜[i]],\n",
    "        U[i] ≥ sum(\n",
    "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : π[j,a[j]] for j in ℐ)\n",
    "            * R[y][i] for (y,a) in enumerate(joint(𝒜))))\n",
    "    @constraint(model, [i=ℐ], sum(π[i,ai] for ai in 𝒜[i]) == 1)\n",
    "    optimize!(model)\n",
    "    πi′(i) = SimpleGamePolicy(𝒫.𝒜[i][ai] => value(π[i,ai]) for ai in 𝒜[i])\n",
    "    return [πi′(i) for i in ℐ]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RockPaperScissors(SimpleGame(0.9, [1, 2], [[:rock, :paper, :scissors], [:rock, :paper, :scissors]], Dict{Tuple{Symbol, Symbol}, Tuple{Real, Real}}((:scissors, :rock) => (-1.0, 1.0), (:rock, :rock) => (0, 0), (:paper, :rock) => (1.0, -1.0), (:scissors, :paper) => (1.0, -1.0), (:rock, :paper) => (-1.0, 1.0), (:paper, :paper) => (0, 0), (:scissors, :scissors) => (0, 0), (:paper, :scissors) => (-1.0, 1.0), (:rock, :scissors) => (1.0, -1.0))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards = Dict{Symbol, Real}(\n",
    "    :rock => 1.0, \n",
    "    :paper => 1.0,\n",
    "    :scissors => 1.0,\n",
    "    )\n",
    "\n",
    "rps = RockPaperScissors(0.9, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a Nash equilibrium for Rock-Paper-Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.14.4, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        6\n",
      "Number of nonzeros in inequality constraint Jacobian.:       24\n",
      "Number of nonzeros in Lagrangian Hessian.............:       15\n",
      "\n",
      "Total number of variables............................:        8\n",
      "                     variables with only lower bounds:        6\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        2\n",
      "Total number of inequality constraints...............:        6\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  0.0000000e+00 9.70e-01 5.00e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  5.3333309e-02 0.00e+00 3.03e+01  -1.7 3.23e-01    -  3.16e-02 1.00e+00f  1\n",
      "   2  7.6033679e-02 0.00e+00 2.00e-07  -1.7 1.14e-02    -  1.00e+00 1.00e+00f  1\n",
      "   3  9.0256948e-04 0.00e+00 1.50e-09  -3.8 3.76e-02    -  1.00e+00 1.00e+00f  1\n",
      "   4  1.1049491e-05 0.00e+00 1.84e-11  -5.7 4.46e-04    -  1.00e+00 1.00e+00f  1\n",
      "   5 -4.9645780e-09 0.00e+00 2.51e-14  -8.6 5.53e-06    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 5\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -4.9645780309158518e-09   -4.9645780309158518e-09\n",
      "Dual infeasibility......:   2.5059035216616534e-14    2.5059035216616534e-14\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.5059036615143538e-09    2.5059036615143538e-09\n",
      "Overall NLP error.......:   2.5059036615143538e-09    2.5059036615143538e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 6\n",
      "Number of objective gradient evaluations             = 6\n",
      "Number of equality constraint evaluations            = 6\n",
      "Number of inequality constraint evaluations          = 6\n",
      "Number of equality constraint Jacobian evaluations   = 6\n",
      "Number of inequality constraint Jacobian evaluations = 6\n",
      "Number of Lagrangian Hessian evaluations             = 5\n",
      "Total seconds in IPOPT                               = 1.783\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{SimpleGamePolicy}:\n",
       " SimpleGamePolicy(Dict(:scissors => 0.33333333333333337, :rock => 0.33333333333333337, :paper => 0.33333333333333337))\n",
       " SimpleGamePolicy(Dict(:scissors => 0.33333333333333337, :rock => 0.33333333333333337, :paper => 0.33333333333333337))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "π = solve(NashEquilibrium(), rps.simpleGame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate 1 match of simple game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simulation (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simulation simple game: play until reward < threshold\n",
    "δ = 1e-16 # threshold\n",
    "\n",
    "function simulation(𝒫::SimpleGame, π::Vector{SimpleGamePolicy})\n",
    "    γ, ℐ, R = 𝒫.γ, 𝒫.ℐ, 𝒫.R\n",
    "    rate = 1.0 # reward rate decrease after each turn\n",
    "    total = [0 for _ in ℐ] # total rewards\n",
    "    # start playing\n",
    "    while true\n",
    "        a = [πi() for πi in π] # random joint action\n",
    "        rw = [r*rate for r in R[a...]]\n",
    "        total = total .+ rw\n",
    "        rate *= γ\n",
    "\n",
    "        # stop if all reward < δ\n",
    "        if all(r -> r < δ, rw)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return total\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonteCarloSimulation (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function MonteCarloSimulation(𝒫::SimpleGame, π::Vector{SimpleGamePolicy}, num_iter::Int)\n",
    "    ℐ = 𝒫.ℐ\n",
    "    total = [0 for _ in ℐ] # total rewards\n",
    "    # play num_iter matchs\n",
    "    for iter = 1 : num_iter\n",
    "        iter_rw = simulation(𝒫, π)\n",
    "        total = total .+ iter_rw\n",
    "    end\n",
    "\n",
    "    println(\"Mean = \", total / num_iter)\n",
    "    println(\"Expected utility = \", [utility(𝒫, π, i) for i in ℐ])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0005446716660989412, -0.0005446716660989412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected utility = [0.0, 0.0]\n",
      " 16.558196 seconds (122.19 M allocations: 5.311 GiB, 6.72% gc time, 13.57% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time MonteCarloSimulation(rps.simpleGame, π, 1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
