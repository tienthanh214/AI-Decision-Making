{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rock-Paper-Scissors\n",
    "\n",
    "Simple Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "if !haskey(Pkg.dependencies(), \"Ipopt\")\n",
    "    Pkg.add(\"Ipopt\")\n",
    "end\n",
    "if !haskey(Pkg.dependencies(), \"JuMP\")\n",
    "    Pkg.add(\"JuMP\")\n",
    "end\n",
    "\n",
    "using JuMP, Ipopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct Simple Game\n",
    "\n",
    "(Source: Algorithms for decision making)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct SimpleGame\n",
    "    Î³  # discount factor\n",
    "    â„  # agents\n",
    "    ð’œ  # joint action space\n",
    "    R  # joint reward function\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct Simple Game Policy and utility calculate function\n",
    "\n",
    "(Source: Algorithms for decision making, modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utility (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "struct SimpleGamePolicy\n",
    "    p # dictionary mapping actions to probabilities\n",
    "\n",
    "    function SimpleGamePolicy(p::Base.Generator)\n",
    "        return SimpleGamePolicy(Dict(p))\n",
    "    end\n",
    "\n",
    "    function SimpleGamePolicy(p::Dict)\n",
    "        vs = collect(values(p))\n",
    "        vs ./= sum(vs)\n",
    "        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))\n",
    "    end\n",
    "\n",
    "    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))\n",
    "end\n",
    "\n",
    "# get probability that policy Ï€i choose action ai\n",
    "(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)\n",
    "\n",
    "# get a random action according to policy Ï€i\n",
    "function (Ï€i::SimpleGamePolicy)()\n",
    "    actions = vec(collect(keys(Ï€i.p)))\n",
    "    probs = vec(collect(values(Ï€i.p)))\n",
    "    return actions[findfirst(cumsum(probs) .> rand())]\n",
    "end\n",
    "\n",
    "# construct joint space\n",
    "joint(X) = vec(collect(Iterators.product(X...)))\n",
    "\n",
    "# function to calculate utility of agent i when playing simple game ð’« with joint policy Ï€\n",
    "function utility(ð’«::SimpleGame, Ï€, i)\n",
    "    ð’œ, R = ð’«.ð’œ, ð’«.R\n",
    "    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))\n",
    "    return sum(R[a][i]*p(a) for a in joint(ð’œ))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rock-Paper-Scissors game\n",
    "\n",
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Symbol}:\n",
       " :rock\n",
       " :paper\n",
       " :scissors"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const N_AGENTS = 2\n",
    "const ACTIONS = [:rock, :paper, :scissors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "struct RockPaperScissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct RockPaperScissors\n",
    "    simpleGame::SimpleGame\n",
    "\n",
    "    function RockPaperScissors(\n",
    "        Î³::Real,                        # discount factor\n",
    "        rewards::Dict{Symbol, Real},    # reward if winning by select an action, opponent's losing reward is negated\n",
    "        )\n",
    "\n",
    "        # construct joint rewards\n",
    "        joint_rewards = construct_joint_rewards(rewards)\n",
    "\n",
    "        simpleGame = SimpleGame(Î³, vec(collect(1:N_AGENTS)), [ACTIONS for _ in 1:N_AGENTS], joint_rewards)\n",
    "\n",
    "        return new(simpleGame)\n",
    "    end\n",
    "\n",
    "    function construct_joint_rewards(rewards::Dict{Symbol, Real})\n",
    "        joint_rewards = Dict{Tuple{Symbol, Symbol}, Tuple{Real, Real}}()\n",
    "\n",
    "        # tie\n",
    "        joint_rewards[(:rock, :rock)] = joint_rewards[(:paper, :paper)] = joint_rewards[(:scissors, :scissors)] = (0, 0)\n",
    "\n",
    "        # rock beats scisssors\n",
    "        joint_rewards[(:rock, :scissors)] = (rewards[:rock], -rewards[:rock])\n",
    "        joint_rewards[(:scissors, :rock)] = (-rewards[:rock], rewards[:rock])\n",
    "\n",
    "        # scisssors beats paper\n",
    "        joint_rewards[(:scissors, :paper)] = (rewards[:scissors], -rewards[:scissors])\n",
    "        joint_rewards[(:paper, :scissors)] = (-rewards[:scissors], rewards[:scissors])\n",
    "\n",
    "        # paper beats rock\n",
    "        joint_rewards[(:paper, :rock)] = (rewards[:paper], -rewards[:paper])\n",
    "        joint_rewards[(:rock, :paper)] = (-rewards[:paper], rewards[:paper])\n",
    "\n",
    "        return joint_rewards\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash Equilibrium\n",
    "\n",
    "(Source: Algorithms for decision making)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solve (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "struct NashEquilibrium end\n",
    "\n",
    "function tensorform(ð’«::SimpleGame)\n",
    "    â„, ð’œ, R = ð’«.â„, ð’«.ð’œ, ð’«.R\n",
    "    â„â€² = eachindex(â„)\n",
    "    ð’œâ€² = [eachindex(ð’œ[i]) for i in â„]\n",
    "    Râ€² = [R[a] for a in joint(ð’œ)]\n",
    "    return â„â€², ð’œâ€², Râ€²\n",
    "end\n",
    "\n",
    "function solve(M::NashEquilibrium, ð’«::SimpleGame)\n",
    "    â„, ð’œ, R = tensorform(ð’«)\n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    @variable(model, U[â„])\n",
    "    @variable(model, Ï€[i=â„, ð’œ[i]] â‰¥ 0)\n",
    "    @NLobjective(model, Min,\n",
    "        sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]\n",
    "            for (y,a) in enumerate(joint(ð’œ))) for i in â„))\n",
    "    @NLconstraint(model, [i=â„, ai=ð’œ[i]],\n",
    "        U[i] â‰¥ sum(\n",
    "            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)\n",
    "            * R[y][i] for (y,a) in enumerate(joint(ð’œ))))\n",
    "    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ð’œ[i]) == 1)\n",
    "    optimize!(model)\n",
    "    Ï€iâ€²(i) = SimpleGamePolicy(ð’«.ð’œ[i][ai] => value(Ï€[i,ai]) for ai in ð’œ[i])\n",
    "    return [Ï€iâ€²(i) for i in â„]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RockPaperScissors(SimpleGame(0.9, [1, 2], [[:rock, :paper, :scissors], [:rock, :paper, :scissors]], Dict{Tuple{Symbol, Symbol}, Tuple{Real, Real}}((:scissors, :rock) => (-1.0, 1.0), (:rock, :rock) => (0, 0), (:paper, :rock) => (1.0, -1.0), (:scissors, :paper) => (1.0, -1.0), (:rock, :paper) => (-1.0, 1.0), (:paper, :paper) => (0, 0), (:scissors, :scissors) => (0, 0), (:paper, :scissors) => (-1.0, 1.0), (:rock, :scissors) => (1.0, -1.0))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards = Dict{Symbol, Real}(\n",
    "    :rock => 1.0, \n",
    "    :paper => 1.0,\n",
    "    :scissors => 1.0,\n",
    "    )\n",
    "\n",
    "rps = RockPaperScissors(0.9, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a Nash equilibrium for Rock-Paper-Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.14.4, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        6\n",
      "Number of nonzeros in inequality constraint Jacobian.:       24\n",
      "Number of nonzeros in Lagrangian Hessian.............:       15\n",
      "\n",
      "Total number of variables............................:        8\n",
      "                     variables with only lower bounds:        6\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        2\n",
      "Total number of inequality constraints...............:        6\n",
      "        inequality constraints with only lower bounds:        6\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  0.0000000e+00 9.70e-01 5.00e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  5.3333309e-02 0.00e+00 3.03e+01  -1.7 3.23e-01    -  3.16e-02 1.00e+00f  1\n",
      "   2  7.6033679e-02 0.00e+00 2.00e-07  -1.7 1.14e-02    -  1.00e+00 1.00e+00f  1\n",
      "   3  9.0256948e-04 0.00e+00 1.50e-09  -3.8 3.76e-02    -  1.00e+00 1.00e+00f  1\n",
      "   4  1.1049491e-05 0.00e+00 1.84e-11  -5.7 4.46e-04    -  1.00e+00 1.00e+00f  1\n",
      "   5 -4.9645780e-09 0.00e+00 2.51e-14  -8.6 5.53e-06    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 5\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -4.9645780309158518e-09   -4.9645780309158518e-09\n",
      "Dual infeasibility......:   2.5059035216616534e-14    2.5059035216616534e-14\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.5059036615143538e-09    2.5059036615143538e-09\n",
      "Overall NLP error.......:   2.5059036615143538e-09    2.5059036615143538e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 6\n",
      "Number of objective gradient evaluations             = 6\n",
      "Number of equality constraint evaluations            = 6\n",
      "Number of inequality constraint evaluations          = 6\n",
      "Number of equality constraint Jacobian evaluations   = 6\n",
      "Number of inequality constraint Jacobian evaluations = 6\n",
      "Number of Lagrangian Hessian evaluations             = 5\n",
      "Total seconds in IPOPT                               = 1.783\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{SimpleGamePolicy}:\n",
       " SimpleGamePolicy(Dict(:scissors => 0.33333333333333337, :rock => 0.33333333333333337, :paper => 0.33333333333333337))\n",
       " SimpleGamePolicy(Dict(:scissors => 0.33333333333333337, :rock => 0.33333333333333337, :paper => 0.33333333333333337))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ï€ = solve(NashEquilibrium(), rps.simpleGame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate 1 match of simple game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simulation (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# simulation simple game: play until reward < threshold\n",
    "Î´ = 1e-16 # threshold\n",
    "\n",
    "function simulation(ð’«::SimpleGame, Ï€::Vector{SimpleGamePolicy})\n",
    "    Î³, â„, R = ð’«.Î³, ð’«.â„, ð’«.R\n",
    "    rate = 1.0 # reward rate decrease after each turn\n",
    "    total = [0 for _ in â„] # total rewards\n",
    "    # start playing\n",
    "    while true\n",
    "        a = [Ï€i() for Ï€i in Ï€] # random joint action\n",
    "        rw = [r*rate for r in R[a...]]\n",
    "        total = total .+ rw\n",
    "        rate *= Î³\n",
    "\n",
    "        # stop if all reward < Î´\n",
    "        if all(r -> r < Î´, rw)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return total\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonteCarloSimulation (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function MonteCarloSimulation(ð’«::SimpleGame, Ï€::Vector{SimpleGamePolicy}, num_iter::Int)\n",
    "    â„ = ð’«.â„\n",
    "    total = [0 for _ in â„] # total rewards\n",
    "    # play num_iter matchs\n",
    "    for iter = 1 : num_iter\n",
    "        iter_rw = simulation(ð’«, Ï€)\n",
    "        total = total .+ iter_rw\n",
    "    end\n",
    "\n",
    "    println(\"Mean = \", total / num_iter)\n",
    "    println(\"Expected utility = \", [utility(ð’«, Ï€, i) for i in â„])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0005446716660989412, -0.0005446716660989412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected utility = [0.0, 0.0]\n",
      " 16.558196 seconds (122.19 M allocations: 5.311 GiB, 6.72% gc time, 13.57% compilation time)\n"
     ]
    }
   ],
   "source": [
    "@time MonteCarloSimulation(rps.simpleGame, Ï€, 1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
